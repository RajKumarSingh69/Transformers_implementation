{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68802340-8646-4fb7-a82d-4f4d0973784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4a55c98-bf70-4f0e-974b-4aae6c69192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we are initalizing the query , key and value vector randomly \n",
    "l,d_k,d_v=4,8,8\n",
    "q=np.random.randn(l,d_k)\n",
    "k=np.random.randn(l,d_k)\n",
    "v=np.random.randn(l,d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "140bedb8-bedd-439a-baec-ce5efee17934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 0.80130725 -0.46483161 -1.98639082 -0.42187247 -0.88257403 -0.15548923\n",
      "   1.15717148 -0.01476848]\n",
      " [-0.20105037  0.41058351 -0.79883495 -0.79533261 -0.53269527 -0.15628997\n",
      "  -1.14339552 -0.11984383]\n",
      " [ 0.71165268 -1.20217112  1.1148881  -0.54266099 -0.3102796  -0.93644262\n",
      "   0.79381267 -0.56538197]\n",
      " [-0.48411164 -0.0961636   0.49070894  0.22509953 -0.51552848 -0.35349724\n",
      "  -1.91916762 -0.03109942]]\n",
      "K\n",
      " [[ 0.93153299 -0.51414795  0.10662418  1.0737366   0.49185312  0.43111192\n",
      "  -1.09719383  1.32047072]\n",
      " [ 0.80399534  1.69758699  1.82957875  3.1199611   1.79123017  1.10054696\n",
      "  -0.96653143 -0.00499251]\n",
      " [-0.83884894  1.74763009  0.13071176 -0.67575595 -1.47111393  1.1627357\n",
      "  -0.11741881 -0.00474525]\n",
      " [ 0.67117666 -0.91776187  0.89453763  1.62047353  2.07038205  0.89828093\n",
      "  -0.60132478 -0.39507005]]\n",
      "V\n",
      " [[ 1.19824388  0.68635123 -0.30974763 -0.38080014 -1.10171085  0.0860364\n",
      "  -1.38004859  2.1638702 ]\n",
      " [-0.62967926  0.7662853  -3.01635049 -0.01370378  0.29032149  1.36633909\n",
      "   0.92538498  0.5315174 ]\n",
      " [-0.12918569  1.14857396 -0.35599435  0.11434539 -0.40233007 -0.15158129\n",
      "  -0.00717955 -0.17260952]\n",
      " [-0.33400323 -0.08913666  0.05362202 -1.61001858 -1.61532621 -0.1492609\n",
      "   0.56744617  1.27861138]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q\\n\",q)\n",
    "print(\"K\\n\",k)\n",
    "print(\"V\\n\",v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae96c9-5287-49ad-a598-c7f66f1f6255",
   "metadata": {},
   "source": [
    "# Self Attention\n",
    "* here we are applying the self attention formula whihc  is Q.K^T/sqrt(dk)+m)v)V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec7f42df-7c0c-4afd-b42a-cabdb661ccb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.46961364, -7.96571422, -0.4773208 , -4.15305079],\n",
       "       [-0.57064863, -3.42803887,  2.05598895, -3.02353784],\n",
       "       [-1.35663944, -3.47273369, -2.90838841, -0.03866653],\n",
       "       [ 1.55116723,  1.59024343,  0.72293845,  0.34850503]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q,k.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "413002c8-fa73-44df-aee4-002ea4f21b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5552160175580109, 1.1698412822885325, 6.5352397977074475)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# why we need sqrt(d_k) in denominator\n",
    "# this is used to minimize the variance which stabilize the value of  the Q.K^T vector \n",
    "q.var(),k.var(),np.matmul(q,k.T).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe527840-c469-4c21-8fd5-e248dbc885dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5552160175580109, 1.1698412822885325, 0.8169049747134309)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in this we are diving the Q.K^T value with sqrt(d_k)\n",
    "scaled=np.matmul(q,k.T)/math.sqrt(d_k)\n",
    "q.var(),k.var(),scaled.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889833e4-eb6b-4198-b46c-bb7c877f3446",
   "metadata": {},
   "source": [
    "# Masking\n",
    "- it is required in decoder part of the transformer neural network. so that we dont look\n",
    "- at a feature word when trying to generate the curent content context word.\n",
    "- This is to ensure words don't get content form word generated in the future\n",
    "- Not required in the encoder, but required in the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d2b9a94-a5b7-4f76-abe8-a530a62527b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask=np.tril(np.ones((l,l)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4204099-7d31-4b7e-a251-04bb5b8085dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[mask==0]=-np.infty\n",
    "mask[mask==1]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fe89dd3-4abd-4e64-b0b4-91a0593f8b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16e03820-811a-411b-a79b-b5cb79b2f895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.51958688,        -inf,        -inf,        -inf],\n",
       "       [-0.20175476, -1.21199477,        -inf,        -inf],\n",
       "       [-0.47964447, -1.22779677, -1.02827058,        -inf],\n",
       "       [ 0.54842043,  0.56223596,  0.25559734,  0.12321514]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled+mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70008f06-9089-46c4-92e6-f6c59183c086",
   "metadata": {},
   "source": [
    "# now we need to pass these into ssoftmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5be17165-9eab-4ccb-ac5d-bb1ad2dea04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T/np.sum(np.exp(x),axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8247095a-77e3-4780-8e96-a6bdec334c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention=softmax(scaled+mask) # we are adding mask becuse i want to focus on recent word only\n",
    "#it means it doesn't focus on next word\n",
    "#here 1 represent let say my is focus on my and in next line 0.733 and 0.266 is focusimg on my name etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9cb4db7-b3e7-47f2-9789-ad855ba5670f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.73306712, 0.26693288, 0.        , 0.        ],\n",
       "       [0.48757104, 0.23073819, 0.28169077, 0.        ],\n",
       "       [0.29293719, 0.29701235, 0.21857629, 0.19147417]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dca7b5e-eb0e-402d-8c18-845e3432b965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.19824388,  0.68635123, -0.30974763, -0.38080014, -1.10171085,\n",
       "         0.0860364 , -1.38004859,  2.1638702 ],\n",
       "       [ 0.71031109,  0.70768826, -1.03222894, -0.28281005, -0.73013165,\n",
       "         0.42779129, -0.76465256,  1.72814156],\n",
       "       [ 0.40254755,  0.83499895, -0.94729156, -0.15661906, -0.58350672,\n",
       "         0.31451642, -0.46137248,  1.1290593 ],\n",
       "       [ 0.07179775,  0.66263767, -1.05417465, -0.39890449, -0.63373607,\n",
       "         0.36931117, -0.02233477,  0.99883798]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new set of matrix that better encapluate with our context word\n",
    "new_v=np.matmul(attention,v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9681ee08-5c7f-4e57-856f-8cf7d443d3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.19824388,  0.68635123, -0.30974763, -0.38080014, -1.10171085,\n",
       "         0.0860364 , -1.38004859,  2.1638702 ],\n",
       "       [-0.62967926,  0.7662853 , -3.01635049, -0.01370378,  0.29032149,\n",
       "         1.36633909,  0.92538498,  0.5315174 ],\n",
       "       [-0.12918569,  1.14857396, -0.35599435,  0.11434539, -0.40233007,\n",
       "        -0.15158129, -0.00717955, -0.17260952],\n",
       "       [-0.33400323, -0.08913666,  0.05362202, -1.61001858, -1.61532621,\n",
       "        -0.1492609 ,  0.56744617,  1.27861138]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32b0835e-1c07-4201-a52e-e4dab00e67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here are all the logics are defined which we practise seprately above\n",
    "def softmax(x):\n",
    "    return (np.exp(x).T/np.sum(np.exp(x),axis=-1)).T\n",
    "\n",
    "#here mask is option becuase the mask in only used in decoder part not in encoder part\n",
    "# so if we are usin this funnction into encoder the we don't pass the mask value and else pass the mask value if we are going to use this funtion insdie the decoder\n",
    "def scaled_dot_product_attention(q,k,v,mask=None):\n",
    "    d_k=q.shape[-1]\n",
    "    scaled=np.matmul(q,k.T)/math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled=scaled+mask\n",
    "    attention=softmax(scaled)\n",
    "    out=np.matmul(attention,v)\n",
    "    return out,attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24866e2c-3211-495c-b244-ec14fa7f544e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 0.80130725 -0.46483161 -1.98639082 -0.42187247 -0.88257403 -0.15548923\n",
      "   1.15717148 -0.01476848]\n",
      " [-0.20105037  0.41058351 -0.79883495 -0.79533261 -0.53269527 -0.15628997\n",
      "  -1.14339552 -0.11984383]\n",
      " [ 0.71165268 -1.20217112  1.1148881  -0.54266099 -0.3102796  -0.93644262\n",
      "   0.79381267 -0.56538197]\n",
      " [-0.48411164 -0.0961636   0.49070894  0.22509953 -0.51552848 -0.35349724\n",
      "  -1.91916762 -0.03109942]]\n",
      "K\n",
      " [[ 0.93153299 -0.51414795  0.10662418  1.0737366   0.49185312  0.43111192\n",
      "  -1.09719383  1.32047072]\n",
      " [ 0.80399534  1.69758699  1.82957875  3.1199611   1.79123017  1.10054696\n",
      "  -0.96653143 -0.00499251]\n",
      " [-0.83884894  1.74763009  0.13071176 -0.67575595 -1.47111393  1.1627357\n",
      "  -0.11741881 -0.00474525]\n",
      " [ 0.67117666 -0.91776187  0.89453763  1.62047353  2.07038205  0.89828093\n",
      "  -0.60132478 -0.39507005]]\n",
      "V\n",
      " [[ 1.19824388  0.68635123 -0.30974763 -0.38080014 -1.10171085  0.0860364\n",
      "  -1.38004859  2.1638702 ]\n",
      " [-0.62967926  0.7662853  -3.01635049 -0.01370378  0.29032149  1.36633909\n",
      "   0.92538498  0.5315174 ]\n",
      " [-0.12918569  1.14857396 -0.35599435  0.11434539 -0.40233007 -0.15158129\n",
      "  -0.00717955 -0.17260952]\n",
      " [-0.33400323 -0.08913666  0.05362202 -1.61001858 -1.61532621 -0.1492609\n",
      "   0.56744617  1.27861138]]\n",
      "New V\n",
      " [[ 0.28269501  0.81159541 -0.37756844 -0.28996176 -0.78038801 -0.0170582\n",
      "  -0.37049786  0.84843561]\n",
      " [ 0.11624897  0.88870956 -0.52988211 -0.1790728  -0.62404127  0.03179079\n",
      "  -0.19068344  0.56952071]\n",
      " [ 0.08049346  0.43092409 -0.50964582 -0.7921126  -1.03466595  0.11173151\n",
      "  -0.01152535  1.19445039]\n",
      " [ 0.07179775  0.66263767 -1.05417465 -0.39890449 -0.63373607  0.36931117\n",
      "  -0.02233477  0.99883798]]\n",
      "Attention\n",
      " [[0.34387169 0.0345895  0.48838162 0.13315719]\n",
      " [0.23173075 0.08438048 0.58653525 0.09735352]\n",
      " [0.27438242 0.12984878 0.15852253 0.43724628]\n",
      " [0.29293719 0.29701235 0.21857629 0.19147417]]\n"
     ]
    }
   ],
   "source": [
    "values,attention=scaled_dot_product_attention(q,k,v,mask=None)\n",
    "print(\"Q\\n\",q)\n",
    "print(\"K\\n\",k)\n",
    "print(\"V\\n\",v)\n",
    "print(\"New V\\n\",values)\n",
    "print(\"Attention\\n\",attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "749122b5-d15d-40b1-919f-0b7a9276ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using these function as decoder part so we need to pass the mask value\n",
    "def softmax(x):\n",
    "    return (np.exp(x).T/np.sum(np.exp(x),axis=-1)).T\n",
    "\n",
    "#here mask is option becuase the mask in only used in decoder part not in encoder part\n",
    "# so if we are usin this funnction into encoder the we don't pass the mask value and else pass the mask value if we are going to use this funtion insdie the decoder\n",
    "def scaled_dot_product_attention(q,k,v,mask=None):\n",
    "    d_k=q.shape[-1]\n",
    "    scaled=np.matmul(q,k.T)/math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled=scaled+mask\n",
    "    attention=softmax(scaled)\n",
    "    out=np.matmul(attention,v)\n",
    "    return out,attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffc83844-f81e-43b0-bfd3-d73a84a60c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 0.80130725 -0.46483161 -1.98639082 -0.42187247 -0.88257403 -0.15548923\n",
      "   1.15717148 -0.01476848]\n",
      " [-0.20105037  0.41058351 -0.79883495 -0.79533261 -0.53269527 -0.15628997\n",
      "  -1.14339552 -0.11984383]\n",
      " [ 0.71165268 -1.20217112  1.1148881  -0.54266099 -0.3102796  -0.93644262\n",
      "   0.79381267 -0.56538197]\n",
      " [-0.48411164 -0.0961636   0.49070894  0.22509953 -0.51552848 -0.35349724\n",
      "  -1.91916762 -0.03109942]]\n",
      "K\n",
      " [[ 0.93153299 -0.51414795  0.10662418  1.0737366   0.49185312  0.43111192\n",
      "  -1.09719383  1.32047072]\n",
      " [ 0.80399534  1.69758699  1.82957875  3.1199611   1.79123017  1.10054696\n",
      "  -0.96653143 -0.00499251]\n",
      " [-0.83884894  1.74763009  0.13071176 -0.67575595 -1.47111393  1.1627357\n",
      "  -0.11741881 -0.00474525]\n",
      " [ 0.67117666 -0.91776187  0.89453763  1.62047353  2.07038205  0.89828093\n",
      "  -0.60132478 -0.39507005]]\n",
      "V\n",
      " [[ 1.19824388  0.68635123 -0.30974763 -0.38080014 -1.10171085  0.0860364\n",
      "  -1.38004859  2.1638702 ]\n",
      " [-0.62967926  0.7662853  -3.01635049 -0.01370378  0.29032149  1.36633909\n",
      "   0.92538498  0.5315174 ]\n",
      " [-0.12918569  1.14857396 -0.35599435  0.11434539 -0.40233007 -0.15158129\n",
      "  -0.00717955 -0.17260952]\n",
      " [-0.33400323 -0.08913666  0.05362202 -1.61001858 -1.61532621 -0.1492609\n",
      "   0.56744617  1.27861138]]\n",
      "New V\n",
      " [[ 1.19824388  0.68635123 -0.30974763 -0.38080014 -1.10171085  0.0860364\n",
      "  -1.38004859  2.1638702 ]\n",
      " [ 0.71031109  0.70768826 -1.03222894 -0.28281005 -0.73013165  0.42779129\n",
      "  -0.76465256  1.72814156]\n",
      " [ 0.40254755  0.83499895 -0.94729156 -0.15661906 -0.58350672  0.31451642\n",
      "  -0.46137248  1.1290593 ]\n",
      " [ 0.07179775  0.66263767 -1.05417465 -0.39890449 -0.63373607  0.36931117\n",
      "  -0.02233477  0.99883798]]\n",
      "Attention\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.73306712 0.26693288 0.         0.        ]\n",
      " [0.48757104 0.23073819 0.28169077 0.        ]\n",
      " [0.29293719 0.29701235 0.21857629 0.19147417]]\n"
     ]
    }
   ],
   "source": [
    "#passing the mask value becuase we use this function as decoder\n",
    "values,attention=scaled_dot_product_attention(q,k,v,mask=mask)\n",
    "print(\"Q\\n\",q)\n",
    "print(\"K\\n\",k)\n",
    "print(\"V\\n\",v)\n",
    "print(\"New V\\n\",values)\n",
    "print(\"Attention\\n\",attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29115e78-176f-411e-b874-639a65de2b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
