{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dedfdb08-5cf9-4195-8a62-181c44855368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af4b7a63-e376-4cfb-9379-7dc0569c88f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_file = r\"F:\\DATA_SCIENCE\\NLP\\Transformers\\dataset\\train.en\"\n",
    "hindi_file = r\"F:\\DATA_SCIENCE\\NLP\\Transformers\\dataset\\train.hi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb232b60-5d27-46e6-9a93-918489454f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2e82f3cc-4787-4fe3-8efe-56e9765191f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = '<START>'\n",
    "PADDING_TOKEN = '<PADDING>'\n",
    "END_TOKEN = '<END>'\n",
    "english_vocabulary = [\n",
    "    START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', '@', \n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', \n",
    "    'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', \n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', \n",
    "    'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\n', '\\t', \n",
    "    PADDING_TOKEN, END_TOKEN\n",
    "]\n",
    "hindi_vocabulary = [\n",
    "    START_TOKEN, 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', \n",
    "    'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', \n",
    "    'व', 'श', 'ष', 'स', 'ह', 'क्ष', 'त्र', 'ज्ञ', 'श्र', 'अं', 'अः', 'ड़', 'ढ़', \n",
    "    ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "    ':', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '\\n', '\\t', \n",
    "    'ॉ', 'ा', 'ि', '्', 'ो', '।', 'े', 'ै', 'ी', 'ं', 'ु', 'ू', 'ौ', 'ँ', '”', '—', '‘', '’', 'ृ', '़', '्', '\\u200d',\n",
    "    'ज़', 'क़', 'ख़', '०', '१', '२', '३', '४', '५', '६', '७', '८', '९', 'ऑ', '“', '”', '\\u200b', '\\u200c', '\\xa0', 'ॅ', \n",
    "    'ा', 'ि', '्', 'ो', '।', 'े', 'ै', 'ी', 'ं', 'ु', 'ू', 'ौ', 'ँ',\n",
    "    '…', 'ː', 'م', 'ي', 'ˌ', 'ل', 'b', 'ك', 'ن', 'æ', 'ة', 'ə', 'n', 'ط', 'ب', 'ا', '₹', '؎', '؋', '؏', 'ھ', 'ں', 'ٹ',\n",
    "    'ٔ', 'ْ', 'ٔ', 'ٔ', 'ُ', 'َ', 'ِ', 'ٍ', 'ً', 'ُ', 'ٰ', 'ۂ', 'ﺵ', 'ﻻ', 'ﺹ', 'ل', 'i', 'u', 'e', 'o', 'a', 'm', 'n',\n",
    "    'r', 's', 't', 'k', 'p', 'x', 'y', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P',\n",
    "    'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "    PADDING_TOKEN, END_TOKEN\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "18822a5e-6d5e-4e77-8dae-3d3e80c3b497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['K', 'U', 'm', 'a', 'r']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'KUmar'\n",
    "list(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eaff55b4-801c-4d82-9c76-44db2acd398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_hindi = {k:v for k,v in enumerate(hindi_vocabulary)}\n",
    "hindi_to_index = {v:k for k,v in enumerate(hindi_vocabulary)}\n",
    "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
    "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0570c132-bd8e-4982-bc91-7b510694c6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(english_file, 'r', encoding='utf-8') as file:\n",
    "    english_sentences = file.readlines()\n",
    "\n",
    "with open(hindi_file, 'r', encoding='utf-8') as file:\n",
    "    hindi_sentences = file.readlines()\n",
    "\n",
    "# Limit Number of sentences\n",
    "TOTAL_SENTENCES = 100000\n",
    "english_sentences = english_sentences[:TOTAL_SENTENCES]\n",
    "hindi_sentences = hindi_sentences[:TOTAL_SENTENCES]\n",
    "\n",
    "english_sentences = [sentence.rstrip('\\n') for sentence in english_sentences]\n",
    "hindi_sentences = [sentence.rstrip('\\n') for sentence in hindi_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "08f12d9b-388e-4329-9435-5c3f8d93f5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"However, Paes, who was partnering Australia's Paul Hanley, could only go as far as the quarterfinals where they lost to Bhupathi and Knowles\",\n",
       " 'Whosoever desires the reward of the world, with Allah is the reward of the world and of the Everlasting Life. Allah is the Hearer, the Seer.',\n",
       " 'The value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness.',\n",
       " 'Mithali To Anchor Indian Team Against Australia in ODIs',\n",
       " 'After the assent of the Honble President on 8thSeptember, 2016, the 101thConstitutional Amendment Act, 2016 came into existence',\n",
       " 'The court has fixed a hearing for February 12',\n",
       " 'Please select the position where the track should be split.',\n",
       " 'As per police, armys 22RR, special operation Group (SOG) of police and the Central Reserve Police Force (CRPF) cordoned the village and launched search operation in the area.',\n",
       " 'Jharkhand chief minister Hemant Soren',\n",
       " 'Arvind Kumar, SHO of the sector 55/56 police station, said a case has been registered under section 376-D (gang rape) of the Indian Penal Code.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3f572abf-620a-441f-900d-32eaeaa010b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।',\n",
       " 'और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है',\n",
       " 'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।',\n",
       " 'आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को',\n",
       " '8 सितम्\\u200dबर, 2016 को माननीय राष्\\u200dट्रपति की स्\\u200dवीकृति मिलने के बाद 101वां संविधान संशोधन अधिनियम, 2016 अस्तित्\\u200dव में आया',\n",
       " 'अदालत ने इस मामले में आगे की सुनवाई के लिए एक फरवरी की तारीख़ तय की',\n",
       " 'जहाँ पर ट्रैक को विभाजित किया जाना है, कृपया वह स्थान चुनें.',\n",
       " 'इसके तुरंत बाद सेना की 22 राष्ट्रीय राइफल्स (आरआर), सीआरपीएफ और पुलिस के स्पेशल ऑपरेशन ग्रुप (एसओजी) के जवानों द्वारा इलाके की घेराबंदी कर तलाशी अभियान चलाया।',\n",
       " 'झारखंड के मुख्यमंत्री हेमंत सोरेन (फोटोः पीटीआई)',\n",
       " 'सेक्टर 55/56 के एसएचओ अरविंद कुमार ने बताया कि इस मामले में आईपीसी की धारा 376-डी (गैंगरेप) के तहत मामला दर्ज कर लिया गया है।']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindi_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a3280378-6873-441c-b202-774c8fa5345b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3270, 2620)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(x) for x in hindi_sentences), max(len(x) for x in english_sentences),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9f5c0d0b-9671-471a-95b9-108c41c9abb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97th percentile length hindi: 258.0\n",
      "97th percentile length English: 267.02999999999884\n"
     ]
    }
   ],
   "source": [
    "PERCENTILE = 97\n",
    "print( f\"{PERCENTILE}th percentile length hindi: {np.percentile([len(x) for x in hindi_sentences], PERCENTILE)}\" )\n",
    "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d7056fb3-a956-4d74-8765-68339ccfc0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 100000\n",
      "Number of valid sentences: 90965\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length = 300\n",
    "\n",
    "def is_valid_tokens(sentence, vocab):\n",
    "    for token in list(set(sentence)):\n",
    "        if token not in vocab:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_valid_length(sentence, max_sequence_length):\n",
    "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
    "\n",
    "valid_sentence_indicies = []\n",
    "for index in range(len(hindi_sentences)):\n",
    "    hindi_sentence, english_sentence = hindi_sentences[index], english_sentences[index]\n",
    "    if is_valid_length(hindi_sentence, max_sequence_length) \\\n",
    "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
    "      and is_valid_tokens(hindi_sentence, hindi_vocabulary):\n",
    "        valid_sentence_indicies.append(index)\n",
    "\n",
    "print(f\"Number of sentences: {len(hindi_sentences)}\")\n",
    "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bd8f4c69-6a53-41ed-9e2a-6f31a5c5d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_sentences = [hindi_sentences[i] for i in valid_sentence_indicies]\n",
    "english_sentences = [english_sentences[i] for i in valid_sentence_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0279bc3e-18b2-4f7a-a765-64ef9adf9db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है',\n",
       " 'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।',\n",
       " 'आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindi_sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "88a88614-b868-48b5-a2c1-4f4f46d8c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, english_sentences, hindi_sentences):\n",
    "        self.english_sentences = english_sentences\n",
    "        self.hindi_sentences = hindi_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.english_sentences[idx], self.hindi_sentences[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "153ffc3a-e820-4a0d-ab95-dc50cbbcb83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset(english_sentences, hindi_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "764fa777-9f88-4b3f-82a7-02ee42b40ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90965"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d214c649-7e3f-447d-b474-4cecfabc9a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness.',\n",
       " 'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bee02464-1842-4ada-9c58-f81f5a69e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3 \n",
    "train_loader = DataLoader(dataset, batch_size)\n",
    "iterator = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e0fda79e-30ae-421f-a48f-54b77c66f4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Whosoever desires the reward of the world, with Allah is the reward of the world and of the Everlasting Life. Allah is the Hearer, the Seer.', 'The value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness.', 'Mithali To Anchor Indian Team Against Australia in ODIs'), ('और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है', 'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।', 'आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को')]\n",
      "[('After the assent of the Honble President on 8thSeptember, 2016, the 101thConstitutional Amendment Act, 2016 came into existence', 'The court has fixed a hearing for February 12', 'Please select the position where the track should be split.'), ('8 सितम्\\u200dबर, 2016 को माननीय राष्\\u200dट्रपति की स्\\u200dवीकृति मिलने के बाद 101वां संविधान संशोधन अधिनियम, 2016 अस्तित्\\u200dव में आया', 'अदालत ने इस मामले में आगे की सुनवाई के लिए एक फरवरी की तारीख़ तय की', 'जहाँ पर ट्रैक को विभाजित किया जाना है, कृपया वह स्थान चुनें.')]\n",
      "[('As per police, armys 22RR, special operation Group (SOG) of police and the Central Reserve Police Force (CRPF) cordoned the village and launched search operation in the area.', 'Arvind Kumar, SHO of the sector 55/56 police station, said a case has been registered under section 376-D (gang rape) of the Indian Penal Code.', \"Briefing media in New Delhi today, party's State-in-charge, Anil Jain said, after the meeting the party will meet the Governor to stake claim for government formation in the state.\"), ('इसके तुरंत बाद सेना की 22 राष्ट्रीय राइफल्स (आरआर), सीआरपीएफ और पुलिस के स्पेशल ऑपरेशन ग्रुप (एसओजी) के जवानों द्वारा इलाके की घेराबंदी कर तलाशी अभियान चलाया।', 'सेक्टर 55/56 के एसएचओ अरविंद कुमार ने बताया कि इस मामले में आईपीसी की धारा 376-डी (गैंगरेप) के तहत मामला दर्ज कर लिया गया है।', 'आज नई दिल्ली में मीडिया से बातचीत में पार्टी के राज्य प्रभारी अनिल जैन ने बताया कि बैठक के बाद पार्टी, सरकार के गठन का दावा पेश करने के लिए राज्यपाल से मिलेगी।')]\n",
      "[('\"Jesus responded, as he taught in the temple, \"\"How is it that the scribes say that the Christ is the son of David?\"', 'Senior leaders of all major parties held electioneering in favour of their candidates.', 'Meanwhile, three people came there on a bike.'), ('फिर यीशु ने मन्दिर में उपदेश करते हुए यह कहा, कि शास्त्री क्योंकर कहते हैं, कि मसीह दाऊद का पुत्रा है?', 'सभी मुख्य पार्टियों के वरिष्ठ नेताओं ने अपने-अपने उम्मीदवारों के पक्ष में चुनाव प्रचार किया।', 'इसी बीच एक बाइक पर तीन लोग आते दिखाई दिए।')]\n",
      "[('Katrina shared the video on Instagram.', 'He does this because he is angry at the alleged desecration of the Indian flag', 'Attempts to contact Randhawa on his mobile over the past one week have remained futile.'), ('कटरीना ने ये वीड\\u200dियो अपनी इंस्\\u200dटास्\\u200dटोरी में शेयर क\\u200dिया है.', 'वह ऐसा इसलिए करता है, क्योंकि वह भारतीय तिरंगे के कथित अनादर से क्रोधित है', 'रंधावा से एक हफ्ते से उनके मोबाईल पर सम्पर्क करने की कोशिश की गयी, लेकिन बात नहीं हो पायी।')]\n"
     ]
    }
   ],
   "source": [
    "for batch_num, batch in enumerate(iterator):\n",
    "    print(batch)\n",
    "    if batch_num > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "356314ea-273d-4f7e-90bc-f152221d0cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence, language_to_index, start_token=True, end_token=True):\n",
    "    sentence_word_indicies = [language_to_index[token] for token in list(sentence)]\n",
    "    if start_token:\n",
    "        sentence_word_indicies.insert(0, language_to_index[START_TOKEN])\n",
    "    if end_token:\n",
    "        sentence_word_indicies.append(language_to_index[END_TOKEN])\n",
    "    for _ in range(len(sentence_word_indicies), max_sequence_length):\n",
    "        sentence_word_indicies.append(language_to_index[PADDING_TOKEN])\n",
    "    return torch.tensor(sentence_word_indicies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "24a25057-d14b-4532-8d4d-081b9f75e468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('They raised slogans against the government and the administration.',\n",
       "  \"Satish, breeder and president of Indian Dog Breeders' Association, said two two-month-old pups, each costing Rs 1 crore, are flying in from China.\",\n",
       "  'The Vice President said that India has consistently voiced the need to have a strong resolute response to terrorism and its manifestations and India is committed to working towards fighting and elimination of this menace.'),\n",
       " ('उन्होंने प्रशासन और सरकार के खिलाफ नारेबाजी की।',\n",
       "  'इंडियन डॉग ब्रीडर असोसिएशन के प्रेज़िडेंट और ब्रीडर सतीश बताते हैं कि उन्होंने चीन से 2 महीने के दो पपीज़ को मंगाया है।',\n",
       "  'उपराष्ट्रपति ने कहा कि भारत ने आतंकवाद के खिलाफ लगातार आवाज उठाई है और इसे खत्म करने के लिए मजबूत तरीके से कार्रवाई करने की जरूरत है। भारत इस खतरे से लड़ने और इसके उन्मूलन के लिए प्रतिबद्ध है।')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915ddfb6-368e-476d-82d9-e11f71884b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "49bc7076-2b36-4184-ae13-a14b36a25bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng_tensor type: <class 'torch.Tensor'>, shape: torch.Size([300])\n",
      "hi_tensor type: <class 'torch.Tensor'>, shape: torch.Size([300])\n",
      "eng_tensor type: <class 'torch.Tensor'>, shape: torch.Size([300])\n",
      "hi_tensor type: <class 'torch.Tensor'>, shape: torch.Size([300])\n",
      "eng_tensor type: <class 'torch.Tensor'>, shape: torch.Size([300])\n",
      "hi_tensor type: <class 'torch.Tensor'>, shape: torch.Size([300])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajkr\\AppData\\Local\\Temp\\ipykernel_18032\\2261546701.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  eng_tokenized = [torch.tensor(tensor) for tensor in eng_tokenized]\n",
      "C:\\Users\\rajkr\\AppData\\Local\\Temp\\ipykernel_18032\\2261546701.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hi_tokenized = [torch.tensor(tensor) for tensor in hi_tokenized]\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists\n",
    "eng_tokenized, hi_tokenized = [], []\n",
    "\n",
    "# Populate lists with tensors\n",
    "for sentence_num in range(batch_size):\n",
    "    eng_sentence, hi_sentence = batch[0][sentence_num], batch[1][sentence_num]\n",
    "    eng_tensor = tokenize(eng_sentence, english_to_index, start_token=False, end_token=False)\n",
    "    hi_tensor = tokenize(hi_sentence, hindi_to_index, start_token=True, end_token=True)\n",
    "    \n",
    "    print(f\"eng_tensor type: {type(eng_tensor)}, shape: {eng_tensor.shape}\")\n",
    "    print(f\"hi_tensor type: {type(hi_tensor)}, shape: {hi_tensor.shape}\")\n",
    "    \n",
    "    eng_tokenized.append(eng_tensor)\n",
    "    hi_tokenized.append(hi_tensor)\n",
    "\n",
    "# Ensure all elements are tensors\n",
    "eng_tokenized = [torch.tensor(tensor) for tensor in eng_tokenized]\n",
    "hi_tokenized = [torch.tensor(tensor) for tensor in hi_tokenized]\n",
    "\n",
    "# Stack the tensors\n",
    "eng_tokenized = torch.stack(eng_tokenized)\n",
    "hi_tokenized = torch.stack(hi_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6353282e-468c-457b-a298-6a9e6feb0d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng_tokenized shape: torch.Size([3, 300])\n",
      "hi_tokenized shape: torch.Size([3, 300])\n"
     ]
    }
   ],
   "source": [
    "print(f\"eng_tokenized shape: {eng_tokenized.shape}\")\n",
    "print(f\"hi_tokenized shape: {hi_tokenized.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bb1b4a60-1bb7-47e1-8441-87a4c8748aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng_tokenized type: torch.int64\n",
      "hi_tokenized type: torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"eng_tokenized type: {eng_tokenized.dtype}\")\n",
    "print(f\"hi_tokenized type: {hi_tokenized.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "def7e385-2f2a-450e-8beb-5cc1294d9580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample English Tokenized Sentence: tensor([52, 72, 69, 89,  1, 82, 65, 73, 83, 69, 68,  1, 83, 76, 79, 71, 65, 78,\n",
      "        83,  1, 65, 71, 65, 73, 78, 83, 84,  1, 84, 72, 69,  1, 71, 79, 86, 69,\n",
      "        82, 78, 77, 69, 78, 84,  1, 65, 78, 68,  1, 84, 72, 69,  1, 65, 68, 77,\n",
      "        73, 78, 73, 83, 84, 82, 65, 84, 73, 79, 78, 15, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99])\n",
      "Sample Hindi Tokenized Sentence: tensor([  0,   5,  31, 141,  44, 142, 147,  31, 144,  53,  32, 141,  38,  41,\n",
      "        139,  43,  31,  53,  11,  38,  53,  43,  38,  12, 139,  38,  53,  12,\n",
      "        144,  53,  13, 140,  39, 139,  33,  53,  31, 139,  38, 144,  34, 139,\n",
      "         19, 146,  53,  12, 146, 143, 242, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243])\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample English Tokenized Sentence:\", eng_tokenized[0])\n",
    "print(\"Sample Hindi Tokenized Sentence:\", hi_tokenized[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "342c30fd-6c92-4d83-83c6-c77934194c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid English tokens: []\n",
      "Invalid Hindi tokens: []\n"
     ]
    }
   ],
   "source": [
    "#Validate Data Integrity\n",
    "invalid_tokens_eng = [token for token in eng_tokenized.flatten().unique() if token not in english_to_index.values()]\n",
    "invalid_tokens_hi = [token for token in hi_tokenized.flatten().unique() if token not in hindi_to_index.values()]\n",
    "\n",
    "print(f\"Invalid English tokens: {invalid_tokens_eng}\")\n",
    "print(f\"Invalid Hindi tokens: {invalid_tokens_hi}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "97618962-ca69-4191-934d-d2377024e83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test English Tokens: tensor([52, 72, 73, 83,  1, 73, 83,  1, 65,  1, 84, 69, 83, 84,  1, 83, 69, 78,\n",
      "        84, 69, 78, 67, 69, 15, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99])\n",
      "Test Hindi Tokens: tensor([  0,  37,  44,  53,   8,  12,  53,  32,  38, 146,  12, 141,  42,  26,\n",
      "         53,  40, 139,  12, 141,  37,  53,  44, 145, 143, 242, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243])\n"
     ]
    }
   ],
   "source": [
    "# Define test sentences\n",
    "test_eng_sentence = \"This is a test sentence.\"\n",
    "test_hi_sentence = \"यह एक परीक्षण वाक्य है।\"\n",
    "\n",
    "# Tokenize manually\n",
    "test_eng_tokens = tokenize(test_eng_sentence, english_to_index, start_token=False, end_token=False)\n",
    "test_hi_tokens = tokenize(test_hi_sentence, hindi_to_index, start_token=True, end_token=True)\n",
    "\n",
    "print(\"Test English Tokens:\", test_eng_tokens)\n",
    "print(\"Test Hindi Tokens:\", test_hi_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c7094737-85f6-4d20-87d2-434be18c6a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample English Tokens: ['T', 'h', 'e', 'y', ' ', 'r', 'a', 'i', 's', 'e', 'd', ' ', 's', 'l', 'o', 'g', 'a', 'n', 's', ' ', 'a', 'g', 'a', 'i', 'n', 's', 't', ' ', 't', 'h', 'e', ' ', 'g', 'o', 'v', 'e', 'r', 'n', 'm', 'e', 'n', 't', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', 'a', 'd', 'm', 'i', 'n', 'i', 's', 't', 'r', 'a', 't', 'i', 'o', 'n', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "Sample Hindi Tokens: ['<START>', 'उ', 'न', '्', 'ह', 'ो', 'ं', 'न', 'े', ' ', 'प', '्', 'र', 'श', 'ा', 'स', 'न', ' ', 'औ', 'र', ' ', 'स', 'र', 'क', 'ा', 'र', ' ', 'क', 'े', ' ', 'ख', 'ि', 'ल', 'ा', 'फ', ' ', 'न', 'ा', 'र', 'े', 'ब', 'ा', 'ज', 'ी', ' ', 'क', 'ी', '।', '<END>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "#Verify Token Mapping:\n",
    "index_to_word_eng = {v: k for k, v in english_to_index.items()}\n",
    "index_to_word_hi = {v: k for k, v in hindi_to_index.items()}\n",
    "\n",
    "sample_eng_tokens = [index_to_word_eng.get(idx.item(), '<UNK>') for idx in eng_tokenized[0]]\n",
    "sample_hi_tokens = [index_to_word_hi.get(idx.item(), '<UNK>') for idx in hi_tokenized[0]]\n",
    "\n",
    "print(\"Sample English Tokens:\", sample_eng_tokens)\n",
    "print(\"Sample Hindi Tokens:\", sample_hi_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "757c8978-a9e4-4121-bfa3-25b4abb6c4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Padding Tokens in English Tensor: 467\n",
      "Number of Padding Tokens in Hindi Tensor: 536\n"
     ]
    }
   ],
   "source": [
    "#Check for Token Distribution:\n",
    "print(\"Number of Padding Tokens in English Tensor:\", (eng_tokenized == english_to_index[PADDING_TOKEN]).sum().item())\n",
    "print(\"Number of Padding Tokens in Hindi Tensor:\", (hi_tokenized == hindi_to_index[PADDING_TOKEN]).sum().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ca7836c2-f39d-42d9-bd3b-e10392c3b5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_INFTY = -1e9\n",
    "\n",
    "def create_masks(eng_batch, hi_batch):\n",
    "    num_sentences = len(eng_batch)\n",
    "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n",
    "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
    "    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "\n",
    "    for idx in range(num_sentences):\n",
    "        eng_sentence_length, hi_sentence_length = len(eng_batch[idx]), len(hi_batch[idx])\n",
    "        eng_chars_to_padding_mask = np.arange(eng_sentence_length, max_sequence_length)\n",
    "        hi_chars_to_padding_mask = np.arange(hi_sentence_length, max_sequence_length)\n",
    "        encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
    "        encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
    "        decoder_padding_mask_self_attention[idx, :, hi_chars_to_padding_mask] = True\n",
    "        decoder_padding_mask_self_attention[idx, hi_chars_to_padding_mask, :] = True\n",
    "        decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
    "        decoder_padding_mask_cross_attention[idx, hi_chars_to_padding_mask, :] = True\n",
    "\n",
    "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
    "    decoder_self_attention_mask = torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
    "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
    "    \n",
    "    print(f\"encoder_self_attention_mask {encoder_self_attention_mask.size()}: {encoder_self_attention_mask[0, :10, :10]}\")\n",
    "    print(f\"decoder_self_attention_mask {decoder_self_attention_mask.size()}: {decoder_self_attention_mask[0, :10, :10]}\")\n",
    "    print(f\"decoder_cross_attention_mask {decoder_cross_attention_mask.size()}: {decoder_cross_attention_mask[0, :10, :10]}\")\n",
    "    \n",
    "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6a269d31-8dfb-4b5d-befc-de08987744a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_self_attention_mask torch.Size([3, 300, 300]): tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "decoder_self_attention_mask torch.Size([3, 300, 300]): tensor([[ 0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [ 0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "decoder_cross_attention_mask torch.Size([3, 300, 300]): tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          ...,\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09]],\n",
       " \n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          ...,\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09]],\n",
       " \n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          ...,\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09]]]),\n",
       " tensor([[[ 0.0000e+00, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          ...,\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09]],\n",
       " \n",
       "         [[ 0.0000e+00, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          ...,\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09]],\n",
       " \n",
       "         [[ 0.0000e+00, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          ...,\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09]]]),\n",
       " tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          ...,\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09]],\n",
       " \n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          ...,\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09]],\n",
       " \n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          ...,\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09],\n",
       "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "           -1.0000e+09, -1.0000e+09]]]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_masks(batch[0], batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d58ff480-ce14-4106-8ea5-26664a2105ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoding[:, :x.size(1)]\n",
    "\n",
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SentenceEmbedding(nn.Module):\n",
    "    \"For a given sentence, create an embedding\"\n",
    "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
    "        super(SentenceEmbedding, self).__init__()\n",
    "        self.vocab_size = len(language_to_index)\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.language_to_index = language_to_index\n",
    "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.START_TOKEN = START_TOKEN\n",
    "        self.END_TOKEN = END_TOKEN\n",
    "        self.PADDING_TOKEN = PADDING_TOKEN\n",
    "    \n",
    "    def batch_tokenize(self, batch, start_token=True, end_token=True):\n",
    "        def tokenize(sentence, start_token=True, end_token=True):\n",
    "            sentence_word_indices = [self.language_to_index[token] for token in list(sentence)]\n",
    "            if start_token:\n",
    "                sentence_word_indices.insert(0, self.language_to_index[self.START_TOKEN])\n",
    "            if end_token:\n",
    "                sentence_word_indices.append(self.language_to_index[self.END_TOKEN])\n",
    "            while len(sentence_word_indices) < self.max_sequence_length:\n",
    "                sentence_word_indices.append(self.language_to_index[self.PADDING_TOKEN])\n",
    "            return torch.tensor(sentence_word_indices)\n",
    "\n",
    "        tokenized = [tokenize(sentence) for sentence in batch]\n",
    "        tokenized = torch.stack(tokenized)\n",
    "        return tokenized.to(get_device())\n",
    "    \n",
    "    def forward(self, x, end_token=True):\n",
    "        x = self.batch_tokenize(x, end_token)\n",
    "        x = self.embedding(x)\n",
    "        pos = self.position_encoder(x).to(get_device())\n",
    "        x = self.dropout(x + pos)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c430bc1-bfef-4aaa-a9b0-06b542d50a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
