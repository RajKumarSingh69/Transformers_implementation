{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6485cc-d1b1-4c50-9598-31b7cb54c69c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Full Transformer Enocder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de02e348-aaba-404e-8db5-b0e539405d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import torch \n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b8b033f-7808-4b29-b452-3317a2d305cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self attention \n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    #q,k,v = 30 x 8 x 200 x 64\n",
    "    d_k = q.size()[-1] # d_k is some constant value wich is sometig value as 64\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)  #dimensions are:- 30 x 8 x 200 x 200 #now we are scaling this accoringto the transformer formulae\n",
    "    # we are transposing the last 2 vectors for key vector . for example :- 1st x=[30,200,512] after x.T.size() it will become [512,200,30] but we don't need this \n",
    "    # so we use transpose(-1,-2) so that we can flip last two things like [30,512,200]\n",
    "    print(f\"scaled.size() : {scaled.size()}\")\n",
    "    if mask is not None:\n",
    "        print(f\"-- ADDING MASK of shape {mask.size()} --\") \n",
    "        #  30 x 8 x 200 x 200 # Broadcasting add. So just the last N dimensions need to match\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1) #dimension:- 30 x 8 x 200 x 200 # here we are passing the scaled value inside softmax , so that we can get probability related to how much we should focus \n",
    "    values = torch.matmul(attention, v)# dim:- 30 x 8 x 200 x 64 #here we are going to multiply our attention and value matrix whihch gives us a new set of vectors or metrix for very single word in which\n",
    "    #it contains a vector for every input word that value vectors will actually have all information associated with context it will know how much attention it need to pay to all of the word in that sentence \n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04b84e36-d1dd-4849-b28c-7abaf3654ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why we need sqrt(d_k) in denominator\n",
    "#here we are initalizing the query , key and value vector randomly \n",
    "l,d_k,d_v=4,8,8\n",
    "q=np.random.randn(l,d_k)\n",
    "k=np.random.randn(l,d_k)\n",
    "v=np.random.randn(l,d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f2b45db-112f-48bd-994b-e4a953366815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[-1.2409909   0.90763738 -0.37162313 -0.72170716  0.37804335 -2.0983542\n",
      "  -0.04451034 -1.77467943]\n",
      " [ 0.4444631  -0.32316202 -0.59587402 -0.86820023 -0.76033911  0.32929205\n",
      "   1.73225249  2.12433348]\n",
      " [-1.73270032 -0.87297009 -2.28696211  2.44771049 -0.5760314  -1.1468115\n",
      "   0.48249086 -0.0833044 ]\n",
      " [ 1.90143069  2.20785939  0.65371143 -1.08809488  0.23084203  0.25644221\n",
      "  -0.99442266 -1.17051757]]\n",
      "K\n",
      " [[-0.38516766  1.08147623 -0.90402383  0.55450851 -0.66211389 -1.60420253\n",
      "  -0.41567107  1.42711086]\n",
      " [-0.30283242  1.21629387  1.0837513  -0.51003737  1.51546112  0.79794109\n",
      "   0.40142596 -1.4081618 ]\n",
      " [ 0.22654161  0.81043765 -0.43465046 -0.3022559  -0.73327951  0.43979501\n",
      "  -0.7688607  -0.74311775]\n",
      " [ 0.55306727  1.75043203 -1.26142238 -0.67548748 -0.20949446 -0.43540319\n",
      "   0.41595423  0.2218635 ]]\n",
      "V\n",
      " [[-0.70733232 -0.01089761 -0.7059525   0.57680686  2.24565716  0.00877778\n",
      "   0.77378761 -1.55469493]\n",
      " [-1.61675356  0.40391201 -1.14248596  1.70354848 -0.21934609 -0.07261548\n",
      "   1.41139726  0.99642613]\n",
      " [ 0.12388114  0.68799595  1.98955749 -1.10124046 -0.9931953  -0.51008776\n",
      "   0.0438553  -0.02853534]\n",
      " [-0.23820393  0.88233421 -0.09490331 -0.18484685 -0.5161435   1.46486027\n",
      "   0.30937529  0.41181861]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q\\n\",q)\n",
    "print(\"K\\n\",k)\n",
    "print(\"V\\n\",v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4808e4f-e20a-411f-b2ec-cf5901b9694e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.99705594,  2.82483175,  0.98707472,  2.28086526],\n",
       "       [ 1.82336713, -3.91616533, -1.84792643,  2.22601388],\n",
       "       [ 5.04970247, -5.74104311, -1.23685558, -0.4527346 ],\n",
       "       [-1.36028532,  5.17656962,  3.84274783,  3.9933698 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q,k.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cce6e10d-eda6-4266-b0c9-6a2adb3df7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.534563270249921, 0.7645613512636953, 9.339336850140846)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var(),k.var(),np.matmul(q,k.T).var() # we can see that query times key is above the 1 or prity large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "116ac4bf-9fe4-4c52-9412-236556087854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.14546082888851875, 0.02312118336159208, 0.9779117511276983)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now see the mean of each\n",
    "q.mean(),k.mean(),np.matmul(q,k.T).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc453d06-bc0d-4b1a-8ccb-6b5250aed3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now when we scaled this then we can see that this query times key\n",
    "#now also becomes has a variance of the order of 1 and its mean also actually has a variance that's slighty it's many time smaller than mean\n",
    "# so that's why we are scaling this by put this into denominator , it allow us our metrices is in mean =0 and std=1 , it allows us us for easier and stable traning\n",
    "# if we can't do this then during the backpropagation and forward propagation our value might be vary leage or small with that our gradient will be affected \n",
    "# and this create vanishing or exploadng grading probeln then our model dosn't learn anything . so for mitigate this problem \n",
    "# we use this sqrt(d_k) in denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e36e2c81-4459-4117-af5c-f9170a6b76ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.534563270249921, 0.7645613512636953, 1.1674171062676058)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled=np.matmul(q,k.T)/math.sqrt(d_k)\n",
    "q.var(),k.var(),scaled.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d8618b1-e89d-4835-b804-76aa30c801c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multihead attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # 512\n",
    "        self.num_heads = num_heads #8\n",
    "        self.head_dim = d_model // num_heads #512/8=64\n",
    "        self.qkv_layer = nn.Linear(d_model , 3 * d_model) #The Linear layer is basically a feed-forward layer , here this is going to be like:- 512 x 1536\n",
    "        self.linear_layer = nn.Linear(d_model, d_model) #512 x 512\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, max_sequence_length, d_model = x.size() # 30 x 200 x 512\n",
    "        print(f\"x.size(): {x.size()}\")\n",
    "        qkv = self.qkv_layer(x) #30*200*1536\n",
    "        print(f\"qkv.size(): {qkv.size()}\") \n",
    "        qkv = qkv.reshape(batch_size, max_sequence_length, self.num_heads, 3 * self.head_dim) # 30*200*8*192. here we are broking our query,key and value vector into 8 heads\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # swith up dimension which is lie 30 x 8 x 200 x 192\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        q, k, v = qkv.chunk(3, dim=-1) #it is going to basically break this entire tensor of this(30*8*200*192) shape into three parts and the way it will going to brake accoring to last dimmension(dim=-1)\n",
    "        #Here each are 30 x 8 x 200 x 64\n",
    "        print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\n",
    "        values, attention = scaled_dot_product(q, k, v, mask) #attention dim:- 30 x 8 x 200 x 20 and value dim:- 30 x 8 x 200 x 64  #here we are performing scaled_dot_product \n",
    "        print(f\"values.size(): {values.size()}, attention.size:{ attention.size()} \") \n",
    "        values = values.reshape(batch_size, max_sequence_length, self.num_heads * self.head_dim) # we are reshaping the value tensor to just be this dimension :- 30 x 200 x 512  \n",
    "        print(f\"values.size(): {values.size()}\")\n",
    "        out = self.linear_layer(values)\n",
    "        print(f\"out.size(): {out.size()}\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3010f186-c52d-472f-b5b8-ee69700fd464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Normalization\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape # 512 # here parameter shape will actually tell us along which dimension we want to perform the layer normalization on and typically this is going to be our embedding dimension   \n",
    "        self.eps=eps # very small value\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))  # 512 #learnable parameter , it is effectively going to represent like a standard deviation of values  \n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape)) # 512 # learnable parameter, it is effectively going to represent like a the mean of values that will be applying , as we know that it will learn continuosly. \n",
    "\n",
    "    def forward(self, inputs):# 30 x 200 x 512\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))] #-1\n",
    "        mean = inputs.mean(dim=dims, keepdim=True) # 30 x 200 x 1  # we are calculating mean for only last dim vector on which we want to perform layer normalization. \n",
    "        print(f\"Mean ({mean.size()})\")\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True) # 30 x 200 x 1 # calculating variance\n",
    "        std = (var + self.eps).sqrt() # 30 x 200 x 1 # calculating standard deviation\n",
    "        print(f\"Standard Deviation  ({std.size()})\")\n",
    "        y = (inputs - mean) / std #30 x 200 x 512 # here we are normalizaing our data mean=0 and std=1 and it only applied on a single sample or most of batch but we want to make sure that these numbers are applicable for accros the traning set and so \n",
    "        #that's kind of y we have learnable parameter gamma  and beta that will kind help us in making sure that we are sacling values y appropraitely so that the eventaul output tensor that we get is going to comparable throughout every single example  \n",
    "        print(f\"y: {y.size()}\")\n",
    "        out = self.gamma * y  + self.beta #30 x 200 x 512, here we have 512 learnable parameters in gamma and beta\n",
    "        print(f\"self.gamma: {self.gamma.size()}, self.beta: {self.beta.size()}\")\n",
    "        print(f\"out: {out.size()}\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44d7a976-c40e-46a3-9b61-68b8ddcabcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden) # 512 x 2048\n",
    "        self.linear2 = nn.Linear(hidden, d_model) # 2048 x 512\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x): # 30 x 200 x 512 \n",
    "        x = self.linear1(x) # 30 x 200 x 2048\n",
    "        print(f\"x after first linear layer: {x.size()}\")\n",
    "        x = self.relu(x) # 30 x 200 x 2048\n",
    "        print(f\"x after activation: {x.size()}\")\n",
    "        x = self.dropout(x) # 30 x 200 x 2048\n",
    "        print(f\"x after dropout: {x.size()}\")\n",
    "        x = self.linear2(x) # 30 x 200 x 512\n",
    "        print(f\"x after 2nd linear layer: {x.size()}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb7606af-68ac-4073-942b-f9029a81bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder layer\n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual_x = x # 30 x 200 x 512\n",
    "        print(\"------- ATTENTION 1 ------\")\n",
    "        x = self.attention(x, mask=None) # 30 x 200 x 512 # in this case we are passing the mask= None because we are make it a Autoregressive model , in which the next model output is depend upone previous prediction\n",
    "        print(\"------- DROPOUT 1 ------\")\n",
    "        x = self.dropout1(x) #30 x 200 x 512\n",
    "        print(\"------- ADD AND LAYER NORMALIZATION 1 ------\")\n",
    "        x = self.norm1(x + residual_x) # 30 x 200 x 512\n",
    "        residual_x = x # 30 x 200 x 512\n",
    "        print(\"------- ATTENTION 2 ------\")\n",
    "        x = self.ffn(x) # 30 x 200 x 512\n",
    "        print(\"------- DROPOUT 2 ------\")\n",
    "        x = self.dropout2(x) # 30 x 200 x 512\n",
    "        print(\"------- ADD AND LAYER NORMALIZATION 2 ------\")\n",
    "        x = self.norm2(x + residual_x) # 30 x 200 x 500 \n",
    "        return x\n",
    "\n",
    "#In the Encoder class we are inheriting the Module class because it inherit all the requirements \n",
    "#which are importing for model learning. it is a base class for all neural netword module\n",
    "# This Module class provide us its own forward method which we implemented below and we need to override this\n",
    "\n",
    "#now in the self.layers:- we are using Sequential which take the inputs one by one. we use * beciase it takes the list and de-construct the list into 5 components\n",
    "# we are doing this 5 times because our num_laeyrs is 5 and we iterate this with the help of loop and then all the thing passing into sequentail, which contains a sequence of 5 encoder layers \n",
    "    class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
    "                                     for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)# it is overriding the forward method because it is necessary to do and it will take x and pass this x inside the 5 encoder layer and then we are returing the layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ae328-807c-4fff-a21f-a754914b8281",
   "metadata": {
    "tags": []
   },
   "source": [
    "# all parameters details:\n",
    "* d_model:- it is the size of every single vector throghout our the encoder architecture \n",
    "* num_head:- it is basically used in multi-head attention mechanism for defining the number of heads for parallel processing the word vectors\n",
    "* drop_prob:-  it is nothin but a dropout which randomly off the status of one or more neurons , which allows the neteork to caputer or understand different aspect parts of the input . with this we can make generalised neural netork.\n",
    "* batch_size:- it is used for faster and stable training.\n",
    "* max_sequence_length:- this is the largest number of words that we can be passing inside the encoder.\n",
    "* ffn_hiden:- we have feed-forward network and for this we are uinsg hindel layer neurons .\n",
    "* num_layers:- it is the number of transformers in encoder unit that we want to include in our architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4fcf3c3-5447-4ef9-904a-3fde9c9b7bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now prepareing data or parameters for encoder\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "batch_size = 30\n",
    "max_sequence_length = 200\n",
    "ffn_hidden = 2048\n",
    "num_layers = 5\n",
    "\n",
    "encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f1ddac8-ea0a-49b3-9686-472c9ee327ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- ATTENTION 1 ------\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size(): torch.Size([30, 200, 1536])\n",
      "qkv.size(): torch.Size([30, 200, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 200, 192])\n",
      "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
      "scaled.size() : torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
      "values.size(): torch.Size([30, 200, 512])\n",
      "out.size(): torch.Size([30, 200, 512])\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "------- ATTENTION 2 ------\n",
      "x after first linear layer: torch.Size([30, 200, 2048])\n",
      "x after activation: torch.Size([30, 200, 2048])\n",
      "x after dropout: torch.Size([30, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "------- ATTENTION 1 ------\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size(): torch.Size([30, 200, 1536])\n",
      "qkv.size(): torch.Size([30, 200, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 200, 192])\n",
      "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
      "scaled.size() : torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
      "values.size(): torch.Size([30, 200, 512])\n",
      "out.size(): torch.Size([30, 200, 512])\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "------- ATTENTION 2 ------\n",
      "x after first linear layer: torch.Size([30, 200, 2048])\n",
      "x after activation: torch.Size([30, 200, 2048])\n",
      "x after dropout: torch.Size([30, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "------- ATTENTION 1 ------\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size(): torch.Size([30, 200, 1536])\n",
      "qkv.size(): torch.Size([30, 200, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 200, 192])\n",
      "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
      "scaled.size() : torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
      "values.size(): torch.Size([30, 200, 512])\n",
      "out.size(): torch.Size([30, 200, 512])\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "------- ATTENTION 2 ------\n",
      "x after first linear layer: torch.Size([30, 200, 2048])\n",
      "x after activation: torch.Size([30, 200, 2048])\n",
      "x after dropout: torch.Size([30, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "------- ATTENTION 1 ------\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size(): torch.Size([30, 200, 1536])\n",
      "qkv.size(): torch.Size([30, 200, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 200, 192])\n",
      "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
      "scaled.size() : torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
      "values.size(): torch.Size([30, 200, 512])\n",
      "out.size(): torch.Size([30, 200, 512])\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "------- ATTENTION 2 ------\n",
      "x after first linear layer: torch.Size([30, 200, 2048])\n",
      "x after activation: torch.Size([30, 200, 2048])\n",
      "x after dropout: torch.Size([30, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "------- ATTENTION 1 ------\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size(): torch.Size([30, 200, 1536])\n",
      "qkv.size(): torch.Size([30, 200, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 200, 192])\n",
      "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
      "scaled.size() : torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
      "values.size(): torch.Size([30, 200, 512])\n",
      "out.size(): torch.Size([30, 200, 512])\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "------- ATTENTION 2 ------\n",
      "x after first linear layer: torch.Size([30, 200, 2048])\n",
      "x after activation: torch.Size([30, 200, 2048])\n",
      "x after dropout: torch.Size([30, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([30, 200, 512])\n"
     ]
    }
   ],
   "source": [
    "# run the encoder\n",
    "x = torch.randn( (batch_size, max_sequence_length, d_model) ) # includes positional encoding\n",
    "out = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27129932-5135-499a-b9d5-4e231c7f735b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
